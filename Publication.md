---
layout: page
title: My Publications
---
<p style="text-align: center;"> This section includes my research, ordered from newest to oldest. </p>
#### 1. Imbalance Learning for Variable Star Classification using Machine Learning
<span style="color:blue"> *Zafiirah Hosenie* </span>, Robert Lyon, Benjamin Stappers, Arrykrishna Mootoovaloo, Vanessa McBride
<p style='text-align: justify;'>The accurate automated classification of variable stars into their respective sub-types is difficult. Machine learning based solutions often fall foul of the imbalanced learning problem, which causes poor generalisation performance in practice, especially on rare variable star sub-types. In previous work, we attempted to overcome such deficiencies via the development of a hierarchical machine learning classifier. This 'algorithm-level' approach to tackling imbalance, yielded promising results on Catalina Real-Time Survey (CRTS) data, outperforming the binary and multi-class classification schemes previously applied in this area. In this work, we attempt to further improve hierarchical classification performance by applying 'data-level' approaches to directly augment the training data so that they better describe under-represented classes. We apply and report results for three data augmentation methods in particular: Randomly Augmented Sampled Light curves from magnitude Error (ğšğ™°ğš‚ğ™»ğ™´), augmenting light curves with Gaussian Process modelling (ğ™¶ğš™ğ™µğš’ğš) and the Synthetic Minority Over-sampling Technique (ğš‚ğ™¼ğ™¾ğšƒğ™´). When combining the 'algorithm-level' (i.e. the hierarchical scheme) together with the 'data-level' approach, we further improve variable star classification accuracy by 1-4%. We found that a higher classification rate is obtained when using ğ™¶ğš™ğ™µğš’ğš in the hierarchical model. Further improvement of the metric scores requires a better standard set of correctly identified variable stars and, perhaps enhanced features are needed. </p>
2. Comparing Multiclass, Binary, and Hierarchical Machine Learning Classification schemes for variable stars
3. DeepSource: Point Source Detection using Deep Learning
4. No evidence for extensions to the standard cosmological model
5. Marginal Likelihoods from Monte Carlo Markov Chains
  * <p style='text-align: justify;'>In this paper, we present a method for computing the marginal likelihood, also known as the model likelihood or Bayesian evidence, from Markov Chain Monte Carlo (MCMC), or other sampled posterior distributions. In order to do this, one needs to be able to estimate the density of points in parameter space, and this can be challenging in high numbers of dimensions. Here we present a Bayesian analysis, where we obtain the posterior for the marginal likelihood, using kth nearest-neighbour distances in parameter space, using the Mahalanobis distance metric, under the assumption that the points in the chain (thinned if required) are independent. We generalise the algorithm to apply to importance-sampled chains, where each point is assigned a weight. We illustrate this with an idealised posterior of known form with an analytic marginal likelihood, and show that for chains of length âˆ¼10^5 points, the technique is effective for parameter spaces with up to âˆ¼20 dimensions. We also argue that k=1 is the optimal choice, and discuss failure modes for the algorithm. In a companion paper (Heavens et al. 2017) we apply the technique to the main MCMC chains from the 2015 Planck analysis of cosmic background radiation data, to infer that quantitatively the simplest 6-parameter flat Î›CDM standard model of cosmology is preferred over all extensions considered. </p>



